<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ajoudaki.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ajoudaki.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-02-15T18:19:13+00:00</updated><id>https://ajoudaki.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">How do you build 1/3 probability with a fair coin?</title><link href="https://ajoudaki.github.io/blog/2024/cool-probability/" rel="alternate" type="text/html" title="How do you build 1/3 probability with a fair coin?" /><published>2024-02-14T00:00:00+00:00</published><updated>2024-02-14T00:00:00+00:00</updated><id>https://ajoudaki.github.io/blog/2024/cool-probability</id><content type="html" xml:base="https://ajoudaki.github.io/blog/2024/cool-probability/"><![CDATA[<p>Here is the question:</p>

<blockquote>
  <p>Suppose you have a fair coin. Given integers $a$ and $b$ where $a&lt; b,$ can you devise a strategy that produces an event with a failure success of $a / b$? You may toss the coin as many times as you like.</p>
</blockquote>

<p>I found the question fascinating for a variety of reasons, But before I go on about various hints and my own thought processes. Please feel free to take as much time as you want before reading more on this problem.</p>

<p>My most immediate thought when thinking about the problem was, well, it is easy enough to build an event with $1/2, 1/4, … $ probability by simply tossing the coin several times and returning success if they all returned head, i.e., AND-ing these events. It also seems plausible we can construct events that are the sum of these values. Namely, $1/2 + 1/4= 3/4.$ Happy with my progress, I started to think of the simplest case that doesn’t fall into this pattern, and $1/3$ struck me as a good choice:</p>

<blockquote>
  <p>Given a fair coin, can you construct an event with $1/3$ probability?</p>
</blockquote>

<p>My vague idea is that the eventual solution must be a clever combination of a number of coin tosses that magically return $1/3$ probability. Despite my hopes, I started to note that most of my attempts ended with events that had a $a / 2^k$ probability of success, which is clearly a pattern that does not fit $1/3.$ My next idea (which, at the time, I thought was very clever) was that the magical combination of events must involve the Bayes rule. Suppose I can devise an event conditioned on another event where the ratios of the two probabilities are $1/3.$ All efforts in this direction were also blocked.</p>

<p>At this point, I was given a hint that there are roughly two approaches to solving this, and one of them involves approximating the event. This immediately kicked off an idea in my head: We can approximate any fractional number by an arbitrary precision in the binary base $\sum_{i=1}^\infty 2^{-k_i} $ where $k_1 &lt; k_2 &lt; \dots $ denote the non-zero binary indices.</p>

<p>Thus, if we’re happy with an “approximation” of $a/b, $ we can approximate the fraction as $a/b \approx \sum_{i=1}^m 2^{-k_i}.$  we can design the following scheme:  Let $E_i$ be an event with $2^{-k_i}$ probability of success, which we know how to construct. My next thought was that we could construct the main event as $E$ as the union of all events $E_i$’s $E = \bigcup_{i=1}^m E_i. $ My thinking was that $P(E) = \sum_{i=1}^m P(E_k) = \sum_{k=1}^m b_i 2^{-k} \approx a/b. $</p>

<p>I wrote down and implemented this idea in Python as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span> 

<span class="c1"># fair coin
</span><span class="k">def</span> <span class="nf">coin</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

<span class="c1"># success of "k" coin tosses (1/2^k) probability
</span><span class="k">def</span> <span class="nf">coin_power</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">all</span><span class="p">([</span><span class="nf">coin</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k</span><span class="p">)])</span>

<span class="c1"># approx a/b prob, B: bits of precision 
</span><span class="k">def</span> <span class="nf">approx_prob</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span> 
    <span class="n">p_int</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="mi">2</span><span class="o">**</span><span class="n">B</span><span class="p">)</span>
    <span class="n">p_bits</span> <span class="o">=</span> <span class="p">[</span><span class="nf">int</span><span class="p">((</span><span class="n">p_int</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span><span class="o">&lt;&lt;</span><span class="n">i</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">B</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">non_zero_bits</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">b</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">p_bits</span><span class="p">)</span> <span class="k">if</span> <span class="n">b</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="nf">any</span><span class="p">([</span><span class="nf">coin_power</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">non_zero_bits</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">test_prob</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">100000</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">([</span><span class="nf">f</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)])</span><span class="o">/</span><span class="n">num_samples</span>

<span class="nf">test_prob</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="nf">approx_prob</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># result = 0.3121 ... 
</span></code></pre></div></div>
<p>To my surprise, the empirical likelihood of the event $0.3121…$ was slightly off from the expected one of $0.3333…$ no matter how many bits of precision and samples I used: A careful consideration reveals a flaw in my original reasoning for $P(E) = \sum_{i=1}^m P(E_i)$: In order to have probability of event equal to sum of individual events, they need to be disjoint: $P(E_i \wedge E_j ) = 0,\forall i\neq j,$ which is not the case if they are independent.</p>

<p>At this point, a hint suggested that <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a> might be a good way to solve this problem. To anyone who is familiar with this concept, this should immediately ring a bell: we can use the coin to define a uniform sampling over any set of power-of-two sizes: for a set of size $2^k$, we can toss $k$ coins and represent them jointly as a binary number $i = \sum_{j=0}^{k-1} i_j\cdot 2^k$ where $i \in [0,2^k-1]$ can be used as the index of the element in the set. We can assume WLOG that $2^k \ge b$. The magic of the rejection sample kicks in here: if we simply reject events $i &gt; b,$ in the remaining events, $i$ is a uniform sample over $0, …, b-1$, which is exactly what we want! However, we are left with some rejections. What should we do? Well, sample again! Here’s a python implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span> 

<span class="c1"># fair coin
</span><span class="k">def</span> <span class="nf">coin</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

<span class="c1"># success of "k" coin tosses (1/2^k) probability
</span><span class="k">def</span> <span class="nf">coin_power</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">all</span><span class="p">([</span><span class="nf">coin</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k</span><span class="p">)])</span>

<span class="k">def</span> <span class="nf">rejection_sampling</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="mi">2</span><span class="o">**</span><span class="n">k</span><span class="o">&lt;</span> <span class="n">b</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">b</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">([</span><span class="nf">int</span><span class="p">(</span><span class="nf">coin</span><span class="p">())</span><span class="o">*</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">k</span><span class="p">)])</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">a</span><span class="p">)</span> 

<span class="k">def</span> <span class="nf">test_prob</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sum</span><span class="p">([</span><span class="nf">f</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)])</span><span class="o">/</span><span class="n">num_samples</span>

<span class="nf">test_prob</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="nf">rejection_sampling</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div></div>

<p>This time, the result $0.333084… $ and the sample mean gets arbitrarily clsoe to $1/3$ as we increase the sample size.</p>

<p>Now, at this point, I was still left with one big daunting question: Is this the most sample-efficient way of going about this problem? In other words, how many random bits does this process consume? Let us assume that $a/b$ is already the smallest fraction ($a$ and $b$’s common denominator is $1$). Clearly, if $b$ is a power of $2$, $b = 2^k$, the process ends deterministically in one step and thus consumes $k = \lceil \log_2(b)\rceil $ random bits. Now, let us assume that $b$ is not a power of two, and we pick the $k$ corresponding to the smallest power of $2.$ In othre words, $b \in (2^{k-1},2^{k}).$ Now, considering that the event is not rejected with probability $b / 2^k,$ we can see that for any value of $b,$ we accept with $ b/ 2^k &gt; 2^{k-1} / 2^k = 1/2$ probability. Thus, on average, we use $2 \lceil \log_2(b)\rceil $ random bits for this process. We can also derive a high-probability bound by observing that the event of having more than $T$ rounds is upper bounded by $2^{-T-1} + 2^{-T-2} + \dots = 2^{-T}.$ Thus, we use at most $T \lceil \log_2(b)\rceil $ bits with $1 - 2^{-T}$ probability.</p>

<p>While this all seemed nice and good, I was still feeling a bit uneasy. More specifically, even for a simple event like $1/3,$ we can’t use a finite number of coin tosses to build this probability! In some sense, I was instinctually convinced that there is some clever way of constructing a finite number of coin tosses that lead to a $1/3$ probability. To my surprise (!), this seems to be impossible. So I sat about proving that it is genuinely impossible to use a <em>finite</em> number of random bits to construct an event with $1/3$ probability. As strong and daunting as that sentence might sound, the proof is extremely simple and straightforward.</p>

<p>In fact, any arbitrary and complex strategy that you take with random bits can be effectively mapped to a binary tree, with each node representing a random bit and the left (right) leaf of each node corresponding to an outcome of that bit (or coin toss). Finally, the leaves of this tree are the places where you designate an outcome (which could be True or False). Upon this abstraction, it is very easy to see that if our strategy guarantees a finite number of random bits or coin tosses, this implies a finite depth for this binary tree. Let’s say the depth of the tree is bounded by $k$. Therefore, probability of any leaf is of the form $2^{-l}$ where $0\le l\le k.$ Since any success leaf corresponds to a disjoint event with a negative power of two probability $2^{-k}$, the overall success probability equals the sum of negative powers of two. This has an immediate and very important revelation: the success probability of the root of the tree can be represented as a floating point in binary basis with at most k bits. For example, any event like $1/4, 3/8, 7/16$, and so on satisfies this. However, $1/3$ is clearly not representable on a binary basis with finite bits. This shows that the construction gave before was actually exactly tight in terms of its worst case rand bit complexity. This completes the proof that $1/3$ event cannot be produced if we only have a finite number of bits.</p>

<p>I thought this was a super cool and interesting observation from an almost ridiculously simple-sounding problem. I hope you enjoyed the read!</p>]]></content><author><name>Amir Joudaki</name></author><category term="cool-facts" /><summary type="html"><![CDATA[Here is the question:]]></summary></entry><entry><title type="html">Cool mathematical facts</title><link href="https://ajoudaki.github.io/blog/2023/cool-facts/" rel="alternate" type="text/html" title="Cool mathematical facts" /><published>2023-06-22T00:00:00+00:00</published><updated>2023-06-22T00:00:00+00:00</updated><id>https://ajoudaki.github.io/blog/2023/cool-facts</id><content type="html" xml:base="https://ajoudaki.github.io/blog/2023/cool-facts/"><![CDATA[<h2 id="matrix-insights">Matrix Insights</h2>

<ul>
  <li>
    <p>Remarkable Fact 1: When $A$ and $B$ are $n\times m$ matrices, both $A^\top B$ and $B^\top A$ exhibit a striking similarity in their reduced spectra. Specifically, if $m&gt;n$, then one of them contains $m-n$ zero eigenvalues. This assertion is grounded in determinant and characteristic polynomials, as demonstrated in this <a href="https://math.stackexchange.com/questions/124888/are-the-eigenvalues-of-ab-equal-to-the-eigenvalues-of-ba">proof</a>.</p>
  </li>
  <li>
    <p>Valuable Linear Algebra Identities:</p>
    <ul>
      <li>Equation 1: \(Tr(A^k)=\sum_{i=1}^n \lambda_i(A)^k\)</li>
      <li>Equation 2: \(Tr((A^*A)^k) = \sum_{i=1}^n \sigma_i(A)^{2k}\)</li>
      <li>Equation 3: 
\(\log\det(A-z I_n)=\sum_{i=1}^n \log|\lambda_i(A)-z|=\sum_{i=1}^n\log|\sigma_i(A-z I)|\)</li>
      <li>Equation 4: 
\(\log\det(A)=\sum_{i=1}^n|dist(X_i,span(X_1,\dots,X_{i-1}))|\)</li>
    </ul>
  </li>
</ul>

<h2 id="mathematical-concepts">Mathematical Concepts</h2>

<ul>
  <li>Lindenbert’s Exchange Method: This method, explained in detail in <a href="https://terrytao.files.wordpress.com/2009/08/random_matrix.pdf">Tao’s presentation</a>, involves two key steps:
    <ol>
      <li>The Gaussian Case: Demonstrating the validity of the law when all underlying random variables are Gaussian.</li>
      <li>Invariance: Showing that the limiting distribution remains unchanged when non-Gaussian random variables are replaced by Gaussian random variables.</li>
    </ol>
  </li>
  <li>
    <p>Weyl Inequality: For symmetric $n\times n$ matrices $S$ and $T$, the Weyl inequality states:
\begin{align}
\max_i |\lambda_i(S) -\lambda_i(T)| \le |S - T|
\end{align}
This inequality offers a powerful tool to bound the eigenvalues of perturbed matrices.</p>
  </li>
  <li>
    <p>Hoffman-Wielandt Inequality: This inequality relates to the deviation of eigenvalues based on the Frobenius norm of deviations:
\begin{align}
\sum_{i=1}^n\ (\lambda_i(A)-\lambda_i(B))^2 \le |B|_F^2
\end{align}
For more information, refer to <a href="https://djalil.chafai.net/blog/2011/12/03/the-hoffman-wielandt-inequality/">Jalil Chafai’s blog</a> and <a href="https://terrytao.wordpress.com/2010/02/02/254a-notes-4-the-semi-circular-law/">Tao’s Blog</a>.</p>
  </li>
  <li>Davis-Kahan Theorem: When $S$ and $T$ are symmetric matrices, and the $i$-th eigenvalue of $S$ is well-separated, the Davis-Kahan theorem states:
\begin{align}
\max_{j\neq i}|\lambda_i(S)-\lambda_j(S)|\ge \delta
\implies \sin\angle(v_i(S),v_i(T)) \le \frac{|S-T|}{\delta}
\end{align}
This result provides insights into the closeness of eigenvectors.</li>
</ul>

<h2 id="convexity-in-symmetric-matrices">Convexity in Symmetric Matrices</h2>

<ul>
  <li>Convexity of Trace Exponential Function: The trace exponential function, defined as $f(X):=\exp\tr(X)$, is convex in the space of symmetric matrices.</li>
</ul>]]></content><author><name>Amir Joudaki</name></author><category term="cool-facts" /><category term="neural-net-theory" /><summary type="html"><![CDATA[An example of a distill-style blog post highlighting key insights]]></summary></entry></feed>