<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ajoudaki.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ajoudaki.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-01-24T15:11:17+00:00</updated><id>https://ajoudaki.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Cool mathematical facts</title><link href="https://ajoudaki.github.io/blog/2023/cool-facts/" rel="alternate" type="text/html" title="Cool mathematical facts" /><published>2023-06-22T00:00:00+00:00</published><updated>2023-06-22T00:00:00+00:00</updated><id>https://ajoudaki.github.io/blog/2023/cool-facts</id><content type="html" xml:base="https://ajoudaki.github.io/blog/2023/cool-facts/"><![CDATA[<h2 id="matrix-insights">Matrix Insights</h2>

<ul>
  <li>
    <p>Remarkable Fact 1: When $A$ and $B$ are $n\times m$ matrices, both $A^\top B$ and $B^\top A$ exhibit a striking similarity in their reduced spectra. Specifically, if $m&gt;n$, then one of them contains $m-n$ zero eigenvalues. This assertion is grounded in determinant and characteristic polynomials, as demonstrated in this <a href="https://math.stackexchange.com/questions/124888/are-the-eigenvalues-of-ab-equal-to-the-eigenvalues-of-ba">proof</a>.</p>
  </li>
  <li>
    <p>Valuable Linear Algebra Identities:</p>
    <ul>
      <li>Equation 1: \(Tr(A^k)=\sum_{i=1}^n \lambda_i(A)^k\)</li>
      <li>Equation 2: \(Tr((A^*A)^k) = \sum_{i=1}^n \sigma_i(A)^{2k}\)</li>
      <li>Equation 3: 
\(\log\det(A-z I_n)=\sum_{i=1}^n \log|\lambda_i(A)-z|=\sum_{i=1}^n\log|\sigma_i(A-z I)|\)</li>
      <li>Equation 4: 
\(\log\det(A)=\sum_{i=1}^n|dist(X_i,span(X_1,\dots,X_{i-1}))|\)</li>
    </ul>
  </li>
</ul>

<h2 id="mathematical-concepts">Mathematical Concepts</h2>

<ul>
  <li>Lindenbert’s Exchange Method: This method, explained in detail in <a href="https://terrytao.files.wordpress.com/2009/08/random_matrix.pdf">Tao’s presentation</a>, involves two key steps:
    <ol>
      <li>The Gaussian Case: Demonstrating the validity of the law when all underlying random variables are Gaussian.</li>
      <li>Invariance: Showing that the limiting distribution remains unchanged when non-Gaussian random variables are replaced by Gaussian random variables.</li>
    </ol>
  </li>
  <li>
    <p>Weyl Inequality: For symmetric $n\times n$ matrices $S$ and $T$, the Weyl inequality states:
\begin{align}
\max_i |\lambda_i(S) -\lambda_i(T)| \le |S - T|
\end{align}
This inequality offers a powerful tool to bound the eigenvalues of perturbed matrices.</p>
  </li>
  <li>
    <p>Hoffman-Wielandt Inequality: This inequality relates to the deviation of eigenvalues based on the Frobenius norm of deviations:
\begin{align}
\sum_{i=1}^n\ (\lambda_i(A)-\lambda_i(B))^2 \le |B|_F^2
\end{align}
For more information, refer to <a href="https://djalil.chafai.net/blog/2011/12/03/the-hoffman-wielandt-inequality/">Jalil Chafai’s blog</a> and <a href="https://terrytao.wordpress.com/2010/02/02/254a-notes-4-the-semi-circular-law/">Tao’s Blog</a>.</p>
  </li>
  <li>Davis-Kahan Theorem: When $S$ and $T$ are symmetric matrices, and the $i$-th eigenvalue of $S$ is well-separated, the Davis-Kahan theorem states:
\begin{align}
\max_{j\neq i}|\lambda_i(S)-\lambda_j(S)|\ge \delta
\implies \sin\angle(v_i(S),v_i(T)) \le \frac{|S-T|}{\delta}
\end{align}
This result provides insights into the closeness of eigenvectors.</li>
</ul>

<h2 id="convexity-in-symmetric-matrices">Convexity in Symmetric Matrices</h2>

<ul>
  <li>Convexity of Trace Exponential Function: The trace exponential function, defined as $f(X):=\exp\tr(X)$, is convex in the space of symmetric matrices.</li>
</ul>]]></content><author><name>Amir Joudaki</name></author><category term="cool-facts" /><category term="neural-net-theory" /><summary type="html"><![CDATA[An example of a distill-style blog post highlighting key insights]]></summary></entry></feed>